{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:25.691383Z",
     "start_time": "2020-11-02T10:51:25.060475Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:28.398172Z",
     "start_time": "2020-11-02T10:51:28.180024Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel=gensim.models.Word2Vec.load(\"weibomodel\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:29.203208Z",
     "start_time": "2020-11-02T10:51:29.184237Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"yqdata.csv\",usecols=['cut','fact','opinion','inhibition','fff',\n",
    "                                      'activation','moral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:37.960374Z",
     "start_time": "2020-11-02T10:51:37.957708Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data=data['cut'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:38.664326Z",
     "start_time": "2020-11-02T10:51:38.660308Z"
    }
   },
   "outputs": [],
   "source": [
    "y_data=data['fact'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:40.479976Z",
     "start_time": "2020-11-02T10:51:40.473992Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_data,\n",
    "                                               y_data,test_size=0.20,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:42.400342Z",
     "start_time": "2020-11-02T10:51:42.387350Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = set(chain(*x_data))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:44.207458Z",
     "start_time": "2020-11-02T10:51:44.203467Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:49.151684Z",
     "start_time": "2020-11-02T10:51:49.146204Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:49.956431Z",
     "start_time": "2020-11-02T10:51:49.952439Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_samples(features, maxlen, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:51.158591Z",
     "start_time": "2020-11-02T10:51:51.154949Z"
    }
   },
   "outputs": [],
   "source": [
    "#参数\n",
    "MAX=20\n",
    "num_epochs = 30\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 32\n",
    "labels = 3\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:52.305087Z",
     "start_time": "2020-11-02T10:51:52.258192Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ea485e14b06c>:7: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n"
     ]
    }
   ],
   "source": [
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:51:54.135898Z",
     "start_time": "2020-11-02T10:51:54.067008Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(x_train),maxlen=MAX))\n",
    "train_labels = torch.tensor(y_train,dtype=torch.int64)\n",
    "test_features = torch.tensor(pad_samples(encode_samples(x_test),maxlen=MAX))\n",
    "test_labels = torch.tensor(y_test,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:01.441175Z",
     "start_time": "2020-11-02T10:52:01.434193Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.FC1 = nn.Linear(num_hiddens * 4, num_hiddens)\n",
    "        else:\n",
    "            self.FC1 = nn.Linear(num_hiddens * 2, num_hiddens)\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Dropout(p=0.5),nn.ReLU(),nn.Linear(num_hiddens,labels))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        tmp = self.FC1(encoding)\n",
    "        outputs=self.decoder(tmp)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:02.348350Z",
     "start_time": "2020-11-02T10:52:02.336756Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, labels, weight, **kwargs):\n",
    "        super(textCNN, self).__init__(**kwargs)\n",
    "        self.labels = labels\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv1 = nn.Conv2d(1, 1, (2, embed_size))\n",
    "        self.conv2 = nn.Conv2d(1, 1, (3, embed_size))\n",
    "        self.conv3 = nn.Conv2d(1, 1, (4, embed_size))\n",
    "        self.pool1 = nn.MaxPool2d((seq_len - 2 + 1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((seq_len - 3 + 1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((seq_len - 4 + 1, 1))\n",
    "        self.linear = nn.Linear(3, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs).view(inputs.shape[0], 1, inputs.shape[1], -1)\n",
    "        x1 = F.relu(self.conv1(inputs))\n",
    "        x2 = F.relu(self.conv2(inputs))\n",
    "        x3 = F.relu(self.conv3(inputs))\n",
    "\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.pool3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        x = x.view(inputs.shape[0], 1, -1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, self.labels)\n",
    "\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:03.355613Z",
     "start_time": "2020-11-02T10:52:03.352621Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#net = textCNN(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "#              seq_len=MAX, labels=labels, weight=weight)\n",
    "#net.to(device)\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:07.178728Z",
     "start_time": "2020-11-02T10:52:04.047474Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:11.608902Z",
     "start_time": "2020-11-02T10:52:11.604913Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:12.531813Z",
     "start_time": "2020-11-02T10:52:12.522842Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T10:52:38.824855Z",
     "start_time": "2020-11-02T10:52:15.425173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.8324, train acc: 0.6156, test loss: 0.8622, test acc: 0.6072, time: 1.10\n",
      "epoch: 1, train loss: 0.8171, train acc: 0.6296, test loss: 0.8638, test acc: 0.6072, time: 0.80\n",
      "epoch: 2, train loss: 0.8185, train acc: 0.6263, test loss: 0.8679, test acc: 0.6072, time: 0.79\n",
      "epoch: 3, train loss: 0.8169, train acc: 0.6288, test loss: 0.8581, test acc: 0.6072, time: 0.77\n",
      "epoch: 4, train loss: 0.8155, train acc: 0.6304, test loss: 0.8597, test acc: 0.6072, time: 0.76\n",
      "epoch: 5, train loss: 0.8165, train acc: 0.6284, test loss: 0.8518, test acc: 0.6072, time: 0.79\n",
      "epoch: 6, train loss: 0.8146, train acc: 0.6293, test loss: 0.8789, test acc: 0.6072, time: 0.76\n",
      "epoch: 7, train loss: 0.8104, train acc: 0.6293, test loss: 0.8633, test acc: 0.6072, time: 0.76\n",
      "epoch: 8, train loss: 0.8089, train acc: 0.6308, test loss: 0.8459, test acc: 0.6090, time: 0.79\n",
      "epoch: 9, train loss: 0.8099, train acc: 0.6243, test loss: 0.8451, test acc: 0.6042, time: 0.76\n",
      "epoch: 10, train loss: 0.8037, train acc: 0.6332, test loss: 0.8462, test acc: 0.6117, time: 0.77\n",
      "epoch: 11, train loss: 0.8028, train acc: 0.6310, test loss: 0.8651, test acc: 0.6072, time: 0.76\n",
      "epoch: 12, train loss: 0.8028, train acc: 0.6324, test loss: 0.8861, test acc: 0.5482, time: 0.79\n",
      "epoch: 13, train loss: 0.7950, train acc: 0.6312, test loss: 1.0035, test acc: 0.6072, time: 0.78\n",
      "epoch: 14, train loss: 0.7955, train acc: 0.6349, test loss: 0.8603, test acc: 0.6122, time: 0.78\n",
      "epoch: 15, train loss: 0.8037, train acc: 0.6275, test loss: 0.8611, test acc: 0.6164, time: 0.77\n",
      "epoch: 16, train loss: 0.7937, train acc: 0.6321, test loss: 0.8378, test acc: 0.6095, time: 0.75\n",
      "epoch: 17, train loss: 0.7980, train acc: 0.6350, test loss: 0.8396, test acc: 0.6152, time: 0.76\n",
      "epoch: 18, train loss: 0.7953, train acc: 0.6390, test loss: 0.8956, test acc: 0.6087, time: 0.74\n",
      "epoch: 19, train loss: 0.7847, train acc: 0.6423, test loss: 0.8610, test acc: 0.5842, time: 0.76\n",
      "epoch: 20, train loss: 0.7791, train acc: 0.6471, test loss: 0.8195, test acc: 0.6119, time: 0.76\n",
      "epoch: 21, train loss: 0.7767, train acc: 0.6420, test loss: 0.8311, test acc: 0.6318, time: 0.78\n",
      "epoch: 22, train loss: 0.7654, train acc: 0.6458, test loss: 0.8177, test acc: 0.6160, time: 0.75\n",
      "epoch: 23, train loss: 0.7497, train acc: 0.6592, test loss: 0.8055, test acc: 0.6157, time: 0.75\n",
      "epoch: 24, train loss: 0.7504, train acc: 0.6554, test loss: 0.8015, test acc: 0.6301, time: 0.78\n",
      "epoch: 25, train loss: 0.7241, train acc: 0.6658, test loss: 0.8029, test acc: 0.6387, time: 0.76\n",
      "epoch: 26, train loss: 0.7156, train acc: 0.6715, test loss: 0.8055, test acc: 0.6265, time: 0.76\n",
      "epoch: 27, train loss: 0.6963, train acc: 0.6817, test loss: 0.8349, test acc: 0.6330, time: 0.77\n",
      "epoch: 28, train loss: 0.6826, train acc: 0.6888, test loss: 0.7492, test acc: 0.6901, time: 0.77\n",
      "epoch: 29, train loss: 0.6863, train acc: 0.6908, test loss: 0.8553, test acc: 0.5699, time: 0.76\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "#         net.train()\n",
    "        net.zero_grad()\n",
    "        feature = feature.to(device)\n",
    "        label = label.to(device)\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "#         scheduler.step()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "#             net.eval()\n",
    "            test_feature = test_feature.cuda()\n",
    "            test_label = test_label.cuda()\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    \n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.4f, test loss: %.4f, test acc: %.4f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
