{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T07:28:27.897328Z",
     "start_time": "2020-11-02T07:28:27.889349Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time \n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T07:28:44.516458Z",
     "start_time": "2020-11-02T07:28:42.950345Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(r'D:\\conda\\envs\\xie\\chinese-bert-wwm-ext')\n",
    "bert = BertModel.from_pretrained(r'D:\\conda\\envs\\xie\\chinese-bert-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:36:56.357361Z",
     "start_time": "2020-11-02T09:36:56.337397Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"yqdata.csv\",usecols=['title','fact','opinion','inhibition','fff',\n",
    "                                      'activation','moral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:36:56.962629Z",
     "start_time": "2020-11-02T09:36:56.956646Z"
    }
   },
   "outputs": [],
   "source": [
    "data['title']=data['title'].apply(lambda x:x[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:36:58.096720Z",
     "start_time": "2020-11-02T09:36:57.544054Z"
    }
   },
   "outputs": [],
   "source": [
    "data['preprocess']=['[CLS] ' + sent + ' [SEP]' for sent in data['title'].values]\n",
    "data['tokenized']=[tokenizer.tokenize(sent) for sent in data['preprocess']]\n",
    "data['original_inputs_id']=[tokenizer.convert_tokens_to_ids(sent) for sent in data['tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:00.887937Z",
     "start_time": "2020-11-02T09:37:00.885304Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad(sentlist,maxlen,PAD=0):\n",
    "    padded_list = sentlist\n",
    "    while(len(padded_list) < maxlen):\n",
    "        padded_list.append(PAD)\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:02.187545Z",
     "start_time": "2020-11-02T09:37:02.182558Z"
    }
   },
   "outputs": [],
   "source": [
    "def mask(sentlist):\n",
    "    attention_mask=[float(i>0) for i in sentlist]\n",
    "    return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:10.046619Z",
     "start_time": "2020-11-02T09:37:10.036904Z"
    }
   },
   "outputs": [],
   "source": [
    "#参数\n",
    "MAX=17\n",
    "data['inputs_id']=data['original_inputs_id'].apply(pad,maxlen=MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:11.465974Z",
     "start_time": "2020-11-02T09:37:11.450452Z"
    }
   },
   "outputs": [],
   "source": [
    "data['attention_mask']=data['inputs_id'].apply(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:14.871600Z",
     "start_time": "2020-11-02T09:37:14.865096Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data=data.sample(frac=0.8,random_state=1)\n",
    "test_data=data[~data.index.isin(train_data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:20.215388Z",
     "start_time": "2020-11-02T09:37:20.206412Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs=torch.tensor(train_data['inputs_id'].tolist(),dtype=torch.int64)\n",
    "train_labels=torch.tensor(train_data['fact'].tolist(),dtype=torch.int64)\n",
    "train_masks=torch.tensor(train_data['attention_mask'].tolist(),dtype=torch.float)\n",
    "test_inputs=torch.tensor(test_data['inputs_id'].tolist(),dtype=torch.int64)\n",
    "test_labels=torch.tensor(test_data['fact'].tolist(),dtype=torch.int64)\n",
    "test_masks=torch.tensor(test_data['attention_mask'].tolist(),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:21.221618Z",
     "start_time": "2020-11-02T09:37:21.215661Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = torch.utils.data.TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_iter = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:23.711660Z",
     "start_time": "2020-11-02T09:37:23.704675Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        #inputs_ids = [batch size, sent len]\n",
    "        input_ids,input_mask,_=batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            bert_out=self.bert(input_ids=input_ids)\n",
    "            \n",
    "            embedded = bert_out[0]\n",
    "              \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:24.667759Z",
     "start_time": "2020-11-02T09:37:24.658190Z"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "lr = 0.05\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:25.462995Z",
     "start_time": "2020-11-02T09:37:25.456014Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:26.366850Z",
     "start_time": "2020-11-02T09:37:26.353832Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:27.160178Z",
     "start_time": "2020-11-02T09:37:27.156672Z"
    }
   },
   "outputs": [],
   "source": [
    "def category_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.max(preds,1)[1]\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:28.066209Z",
     "start_time": "2020-11-02T09:37:28.062220Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:28.787989Z",
     "start_time": "2020-11-02T09:37:28.777045Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids,input_mask,labels=batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = category_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator),epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:37:29.982161Z",
     "start_time": "2020-11-02T09:37:29.976175Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids,input_mask,labels=batch\n",
    "\n",
    "            predictions = model(batch).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = category_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:38:19.244152Z",
     "start_time": "2020-11-02T09:37:30.885818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.423 | Train Acc: 51.97%\n",
      "\t Val. Loss: 0.814 |  Val. Acc: 64.45%\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.145 | Train Acc: 52.38%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 64.45%\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.173 | Train Acc: 51.44%\n",
      "\t Val. Loss: 1.224 |  Val. Acc: 31.36%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.235 | Train Acc: 52.02%\n",
      "\t Val. Loss: 1.030 |  Val. Acc: 64.45%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.181 | Train Acc: 52.89%\n",
      "\t Val. Loss: 0.946 |  Val. Acc: 64.45%\n",
      "Epoch: 06 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.123 | Train Acc: 52.49%\n",
      "\t Val. Loss: 0.824 |  Val. Acc: 64.45%\n",
      "Epoch: 07 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.180 | Train Acc: 54.59%\n",
      "\t Val. Loss: 1.242 |  Val. Acc: 64.45%\n",
      "Epoch: 08 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.019 | Train Acc: 52.30%\n",
      "\t Val. Loss: 1.000 |  Val. Acc: 64.45%\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.044 | Train Acc: 55.94%\n",
      "\t Val. Loss: 1.800 |  Val. Acc: 31.36%\n",
      "Epoch: 10 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.281 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.798 |  Val. Acc: 64.45%\n",
      "Epoch: 11 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.084 | Train Acc: 52.68%\n",
      "\t Val. Loss: 1.239 |  Val. Acc: 31.36%\n",
      "Epoch: 12 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.049 | Train Acc: 55.51%\n",
      "\t Val. Loss: 0.935 |  Val. Acc: 64.45%\n",
      "Epoch: 13 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.042 | Train Acc: 53.66%\n",
      "\t Val. Loss: 0.810 |  Val. Acc: 64.45%\n",
      "Epoch: 14 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.132 | Train Acc: 52.26%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 64.45%\n",
      "Epoch: 15 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.257 | Train Acc: 51.38%\n",
      "\t Val. Loss: 1.004 |  Val. Acc: 64.45%\n",
      "Epoch: 16 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.108 | Train Acc: 52.60%\n",
      "\t Val. Loss: 0.963 |  Val. Acc: 31.36%\n",
      "Epoch: 17 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.163 | Train Acc: 52.57%\n",
      "\t Val. Loss: 0.868 |  Val. Acc: 64.45%\n",
      "Epoch: 18 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.152 | Train Acc: 52.69%\n",
      "\t Val. Loss: 0.942 |  Val. Acc: 64.45%\n",
      "Epoch: 19 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.165 | Train Acc: 54.23%\n",
      "\t Val. Loss: 2.071 |  Val. Acc: 64.45%\n",
      "Epoch: 20 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.227 | Train Acc: 52.55%\n",
      "\t Val. Loss: 0.944 |  Val. Acc: 64.45%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss,train_acc= train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss,valid_acc = evaluate(model, test_iter, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
